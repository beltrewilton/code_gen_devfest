{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9b614d-9d7e-4c72-918d-5cccc54abb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eed96a4-e0ec-48eb-bb58-a56bbe8fefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "# The instruction dataset to use\n",
    "# dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "dataset_name = \"mbpp\"\n",
    "#dataset_name = \"HuggingFaceH4/CodeAlpaca_20K\"\n",
    "# Dataset split\n",
    "dataset_split= \"train\"\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-int4-python-code-20k\"\n",
    "# Huggingface repository\n",
    "hf_model_repo=\"edumunozsala/\"+new_model\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"mps\": 0}\n",
    "device_map = \"auto\"\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_double_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = new_model\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 2\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1 # 2\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4 #1e-5\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\" #\"constant\"\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = False\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "# Disable tqdm\n",
    "disable_tqdm= False\n",
    "\n",
    "################################################################################\n",
    "# SFTTrainer parameters\n",
    "################################################################################\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 2048 #None\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = True #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493601b4-e644-45b1-aa2d-ca78c9dcf86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# Log in to HF Hub\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca7fedc-6d18-458e-94b9-986d8f30659d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8938c64c4fc64b51b214167ef4a1ef60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b194d084bec14db3901f1f0c617e6681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241141d5cf374baf839c5491e5226f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7406772730bb491fb5a7b75386e72dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/131k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa61e191b2a41cd8f567692a3dda17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/374 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8e6036c5ca4ce09f232516b9f667b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa591a53ea9142018e8c43587e970c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb587742fb7408fb529dcb88115f583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating prompt split:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 374\n",
      "{'task_id': 926, 'text': 'Write a function to find n-th rencontres number.', 'code': 'def binomial_coeffi(n, k): \\r\\n\\tif (k == 0 or k == n): \\r\\n\\t\\treturn 1\\r\\n\\treturn (binomial_coeffi(n - 1, k - 1) \\r\\n\\t\\t+ binomial_coeffi(n - 1, k)) \\r\\ndef rencontres_number(n, m): \\r\\n\\tif (n == 0 and m == 0): \\r\\n\\t\\treturn 1\\r\\n\\tif (n == 1 and m == 0): \\r\\n\\t\\treturn 0\\r\\n\\tif (m == 0): \\r\\n\\t\\treturn ((n - 1) * (rencontres_number(n - 1, 0)+ rencontres_number(n - 2, 0))) \\r\\n\\treturn (binomial_coeffi(n, m) * rencontres_number(n - m, 0))', 'test_list': ['assert rencontres_number(7, 2) == 924', 'assert rencontres_number(3, 0) == 2', 'assert rencontres_number(3, 1) == 3'], 'test_setup_code': '', 'challenge_test_list': []}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_name, split=dataset_split)\n",
    "# Show dataset size\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "# Show an example\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786e1e0b-e397-4969-98df-bddbf320fbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'challenge_test_list': [],\n",
      " 'code': 'def factorial(start,end): \\r\\n'\n",
      "         '    res = 1 \\r\\n'\n",
      "         '    for i in range(start,end + 1): \\r\\n'\n",
      "         '        res *= i      \\r\\n'\n",
      "         '    return res \\r\\n'\n",
      "         'def sum_of_square(n): \\r\\n'\n",
      "         '   return int(factorial(n + 1, 2 * n)  /factorial(1, n)) ',\n",
      " 'task_id': 905,\n",
      " 'test_list': ['assert sum_of_square(4) == 70',\n",
      "               'assert sum_of_square(5) == 252',\n",
      "               'assert sum_of_square(2) == 6'],\n",
      " 'test_setup_code': '',\n",
      " 'text': 'Write a python function to find the sum of squares of binomial co-efficients.'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# Show a random example\n",
    "pprint.pprint(dataset[randrange(len(dataset))], width=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7f36d0-3974-43e8-9570-fefe077ac05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the instruction format for Mostly Basic Python Problems (mbpp)\n",
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
    "\n",
    "### Task:\n",
    "{sample['text']}\n",
    "\n",
    "### Input:\n",
    "{sample['test_list'][randrange(len(sample['test_list']))]}\n",
    "\n",
    "### Response:\n",
    "{sample['code']}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
    "\n",
    "### Task:\n",
    "{sample['text']}\n",
    "\n",
    "### Response:\n",
    "{sample['code']}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af005ff-e215-4303-b9cc-38ebfb0fef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
      "\n",
      "### Task:\n",
      "Write a python function to shift first element to the end of given list.\n",
      "\n",
      "### Response:\n",
      "def move_last(num_list):\n",
      "    a = [num_list[0] for i in range(num_list.count(num_list[0]))]\n",
      "    x = [ i for i in num_list if i != num_list[0]]\n",
      "    x.extend(a)\n",
      "    return (x)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show a formatted instruction\n",
    "print(format_instruction(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ffcdc5-152d-404d-9b4d-682329fa82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the type\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_use_double_quant=use_double_nested_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df18ad7-920e-442c-9707-201488d66690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache = False, device_map=device_map)\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9a140-9610-495a-94e3-96258f69029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Not necessary when using SFTTrainer\n",
    "# prepare model for training\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, peft_config)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd7b58-a372-45a2-aafc-0bf9e57b0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size, # 6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    optim=optim,\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    #max_steps=max_steps,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    disable_tqdm=disable_tqdm,\n",
    "    report_to=\"tensorboard\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf51b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=packing,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf72500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "\n",
    "# save model in local\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f05ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() # PyTorch thing???\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the trained and saved model and merge it then we can save the whole model\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = new_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the merged model\n",
    "\n",
    "\n",
    "sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
    "\n",
    "### Task:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"\\nGenerated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"\\nGround truth:\\n{sample['output']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an new inference\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Prompt:\\n{sample['output']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample['instruction']}\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9300a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674811de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d8e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
